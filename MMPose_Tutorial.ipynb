{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F77yOqgkX8p4"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/open-mmlab/mmpose/blob/dev-1.x/demo/MMPose_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8xX3YewOtqV0"
      },
      "source": [
        "# MMPose Tutorial\n",
        "\n",
        "Welcome to MMPose colab tutorial! In this tutorial, we will show you how to\n",
        "\n",
        "- install MMPose 1.x\n",
        "- perform inference with an MMPose model\n",
        "- train a new mmpose model with your own datasets\n",
        "\n",
        "Let's start!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bkw-kUD8t3t8"
      },
      "source": [
        "## Install MMPose\n",
        "\n",
        "We recommend to use a conda environment to install mmpose and its dependencies. And compilers `nvcc` and `gcc` are required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f_Ebb2otWtd",
        "outputId": "8c16b8ae-b927-41d5-c49e-d61ba6798a2d"
      },
      "outputs": [],
      "source": [
        "# check NVCC version\n",
        "!nvcc -V\n",
        "\n",
        "# check GCC version\n",
        "!gcc --version\n",
        "\n",
        "# check python in conda environment\n",
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igSm4jhihE2M",
        "outputId": "0d521640-a4d7-4264-889c-df862e9c332f"
      },
      "outputs": [],
      "source": [
        "# install dependencies: (if your colab has CUDA 11.8)\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLcoZr3ot9iw",
        "outputId": "70e5d18e-746c-41a3-a761-6303b79eaf02"
      },
      "outputs": [],
      "source": [
        "# install MMEngine, MMCV and MMDetection using MIM\n",
        "%pip install -U openmim\n",
        "!mim install mmengine\n",
        "!mim install \"mmcv>=2.0.0\"\n",
        "!mim install \"mmdet>=3.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42hRcloJhE2N",
        "outputId": "9175e011-82c0-438d-f378-264e8467eb09"
      },
      "outputs": [],
      "source": [
        "# for better Colab compatibility, install xtcocotools from source\n",
        "%pip install git+https://github.com/jin-s13/xtcocoapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzuSKOjMvJZu",
        "outputId": "d6a7a3f8-2d96-40a6-a7c4-65697e18ffc9"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/open-mmlab/mmpose.git\n",
        "# The master branch is version 1.x \n",
        "%cd mmpose\n",
        "%pip install -r requirements.txt\n",
        "%pip install -v -e .\n",
        "# \"-v\" means verbose, or more output\n",
        "# \"-e\" means installing a project in editable mode,\n",
        "# thus any local modifications made to the code will take effect without reinstallation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Miy2zVRcw6kL",
        "outputId": "1cbae5a0-249a-4cb2-980a-7db592c759da"
      },
      "outputs": [],
      "source": [
        "# Check Pytorch installation\n",
        "import torch, torchvision\n",
        "\n",
        "print('torch version:', torch.__version__, torch.cuda.is_available())\n",
        "print('torchvision version:', torchvision.__version__)\n",
        "\n",
        "# Check MMPose installation\n",
        "import mmpose\n",
        "\n",
        "print('mmpose version:', mmpose.__version__)\n",
        "\n",
        "# Check mmcv installation\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "\n",
        "print('cuda version:', get_compiling_cuda_version())\n",
        "print('compiler information:', get_compiler_version())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r2bf94XpyFnk"
      },
      "source": [
        "## Inference with an MMPose model\n",
        "\n",
        "MMPose provides high-level APIs for model inference and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjTt4LZAx_lK",
        "outputId": "485b62c4-226b-45fb-a864-99c2a029353c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\" to /home/forest/.cache/torch/hub/checkpoints/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
            "/home/forest/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
            "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
          ]
        }
      ],
      "source": [
        "import mmcv\n",
        "from mmcv import imread\n",
        "import mmengine\n",
        "from mmengine.registry import init_default_scope\n",
        "import numpy as np\n",
        "\n",
        "from mmpose.apis import inference_topdown\n",
        "from mmpose.apis import init_model as init_pose_estimator\n",
        "from mmpose.evaluation.functional import nms\n",
        "from mmpose.registry import VISUALIZERS\n",
        "from mmpose.structures import merge_data_samples\n",
        "\n",
        "try:\n",
        "    from mmdet.apis import inference_detector, init_detector\n",
        "    has_mmdet = True\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    has_mmdet = False\n",
        "\n",
        "local_runtime = False\n",
        "\n",
        "try:\n",
        "    from google.colab.patches import cv2_imshow  # for image visualization in colab\n",
        "except:\n",
        "    local_runtime = True\n",
        "\n",
        "img = 'tests/data/coco/000000197388.jpg'\n",
        "pose_config = 'configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192.py'\n",
        "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'\n",
        "det_config = 'demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py'\n",
        "det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
        "\n",
        "device = 'cuda:0'\n",
        "cfg_options = dict(model=dict(test_cfg=dict(output_heatmaps=True)))\n",
        "\n",
        "\n",
        "# build detector\n",
        "detector = init_detector(\n",
        "    det_config,\n",
        "    det_checkpoint,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "# build pose estimator\n",
        "pose_estimator = init_pose_estimator(\n",
        "    pose_config,\n",
        "    pose_checkpoint,\n",
        "    device=device,\n",
        "    cfg_options=cfg_options\n",
        ")\n",
        "\n",
        "# init visualizer\n",
        "pose_estimator.cfg.visualizer.radius = 3\n",
        "pose_estimator.cfg.visualizer.line_width = 1\n",
        "visualizer = VISUALIZERS.build(pose_estimator.cfg.visualizer)\n",
        "# the dataset_meta is loaded from the checkpoint and\n",
        "# then pass to the model in init_pose_estimator\n",
        "visualizer.set_dataset_meta(pose_estimator.dataset_meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tsSM0NRPEG1Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_img(img_path, detector, pose_estimator, visualizer,\n",
        "                  show_interval, out_file):\n",
        "    \"\"\"Visualize predicted keypoints (and heatmaps) of one image.\"\"\"\n",
        "\n",
        "    # predict bbox\n",
        "    scope = detector.cfg.get('default_scope', 'mmdet')\n",
        "    if scope is not None:\n",
        "        init_default_scope(scope)\n",
        "    detect_result = inference_detector(detector, img_path)\n",
        "    pred_instance = detect_result.pred_instances.cpu().numpy()\n",
        "    bboxes = np.concatenate(\n",
        "        (pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
        "    bboxes = bboxes[np.logical_and(pred_instance.labels == 0,\n",
        "                                   pred_instance.scores > 0.3)]\n",
        "    bboxes = bboxes[nms(bboxes, 0.3)][:, :4]\n",
        "\n",
        "    # predict keypoints\n",
        "    pose_results = inference_topdown(pose_estimator, img_path, bboxes)\n",
        "    data_samples = merge_data_samples(pose_results)\n",
        "\n",
        "    # show the results\n",
        "    img = mmcv.imread(img_path, channel_order='rgb')\n",
        "\n",
        "    visualizer.add_datasample(\n",
        "        'result',\n",
        "        img,\n",
        "        data_sample=data_samples,\n",
        "        draw_gt=False,\n",
        "        draw_heatmap=True,\n",
        "        draw_bbox=True,\n",
        "        show=False,\n",
        "        wait_time=show_interval,\n",
        "        out_file=out_file,\n",
        "        kpt_thr=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogj5h9x-HiMA",
        "outputId": "71452169-c16a-4a61-b558-f7518fcefaa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "08/10 17:56:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The current default scope \"mmpose\" is not \"mmdet\", `init_default_scope` will force set the currentdefault scope to \"mmdet\".\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "nms_impl: implementation for device cuda:0 not found.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m visualize_img(\n\u001b[1;32m      2\u001b[0m     img,\n\u001b[1;32m      3\u001b[0m     detector,\n\u001b[1;32m      4\u001b[0m     pose_estimator,\n\u001b[1;32m      5\u001b[0m     visualizer,\n\u001b[1;32m      6\u001b[0m     show_interval\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     out_file\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m      9\u001b[0m vis_result \u001b[39m=\u001b[39m visualizer\u001b[39m.\u001b[39mget_image()\n",
            "Cell \u001b[0;32mIn[8], line 9\u001b[0m, in \u001b[0;36mvisualize_img\u001b[0;34m(img_path, detector, pose_estimator, visualizer, show_interval, out_file)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m scope \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     init_default_scope(scope)\n\u001b[0;32m----> 9\u001b[0m detect_result \u001b[39m=\u001b[39m inference_detector(detector, img_path)\n\u001b[1;32m     10\u001b[0m pred_instance \u001b[39m=\u001b[39m detect_result\u001b[39m.\u001b[39mpred_instances\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     11\u001b[0m bboxes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(\n\u001b[1;32m     12\u001b[0m     (pred_instance\u001b[39m.\u001b[39mbboxes, pred_instance\u001b[39m.\u001b[39mscores[:, \u001b[39mNone\u001b[39;00m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet/apis/inference.py:189\u001b[0m, in \u001b[0;36minference_detector\u001b[0;34m(model, imgs, test_pipeline, text_prompt, custom_entities)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[39m# forward the model\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 189\u001b[0m         results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtest_step(data_)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    191\u001b[0m     result_list\u001b[39m.\u001b[39mappend(results)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batch:\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/model/base_model/base_model.py:145\u001b[0m, in \u001b[0;36mBaseModel.test_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"``BaseModel`` implements ``test_step`` the same as ``val_step``.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39m    list: The predictions of given data.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_preprocessor(data, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_forward(data, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpredict\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/model/base_model/base_model.py:340\u001b[0m, in \u001b[0;36mBaseModel._run_forward\u001b[0;34m(self, data, mode)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Unpacks data for :meth:`forward`\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39m    dict or list: Results of training or testing mode.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 340\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata, mode\u001b[39m=\u001b[39;49mmode)\n\u001b[1;32m    341\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    342\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\u001b[39m*\u001b[39mdata, mode\u001b[39m=\u001b[39mmode)\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet/models/detectors/base.py:94\u001b[0m, in \u001b[0;36mBaseDetector.forward\u001b[0;34m(self, inputs, data_samples, mode)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(inputs, data_samples)\n\u001b[1;32m     93\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(inputs, data_samples)\n\u001b[1;32m     95\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtensor\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(inputs, data_samples)\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet/models/detectors/two_stage.py:231\u001b[0m, in \u001b[0;36mTwoStageDetector.predict\u001b[0;34m(self, batch_inputs, batch_data_samples, rescale)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m# If there are no pre-defined proposals, use RPN to get proposals\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m batch_data_samples[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mproposals\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     rpn_results_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrpn_head\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    232\u001b[0m         x, batch_data_samples, rescale\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    233\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     rpn_results_list \u001b[39m=\u001b[39m [\n\u001b[1;32m    235\u001b[0m         data_sample\u001b[39m.\u001b[39mproposals \u001b[39mfor\u001b[39;00m data_sample \u001b[39min\u001b[39;00m batch_data_samples\n\u001b[1;32m    236\u001b[0m     ]\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet/models/dense_heads/base_dense_head.py:197\u001b[0m, in \u001b[0;36mBaseDenseHead.predict\u001b[0;34m(self, x, batch_data_samples, rescale)\u001b[0m\n\u001b[1;32m    191\u001b[0m batch_img_metas \u001b[39m=\u001b[39m [\n\u001b[1;32m    192\u001b[0m     data_samples\u001b[39m.\u001b[39mmetainfo \u001b[39mfor\u001b[39;00m data_samples \u001b[39min\u001b[39;00m batch_data_samples\n\u001b[1;32m    193\u001b[0m ]\n\u001b[1;32m    195\u001b[0m outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x)\n\u001b[0;32m--> 197\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_by_feat(\n\u001b[1;32m    198\u001b[0m     \u001b[39m*\u001b[39;49mouts, batch_img_metas\u001b[39m=\u001b[39;49mbatch_img_metas, rescale\u001b[39m=\u001b[39;49mrescale)\n\u001b[1;32m    199\u001b[0m \u001b[39mreturn\u001b[39;00m predictions\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet/models/dense_heads/base_dense_head.py:279\u001b[0m, in \u001b[0;36mBaseDenseHead.predict_by_feat\u001b[0;34m(self, cls_scores, bbox_preds, score_factors, batch_img_metas, cfg, rescale, with_nms)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m         score_factor_list \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_levels)]\n\u001b[0;32m--> 279\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_by_feat_single(\n\u001b[1;32m    280\u001b[0m         cls_score_list\u001b[39m=\u001b[39;49mcls_score_list,\n\u001b[1;32m    281\u001b[0m         bbox_pred_list\u001b[39m=\u001b[39;49mbbox_pred_list,\n\u001b[1;32m    282\u001b[0m         score_factor_list\u001b[39m=\u001b[39;49mscore_factor_list,\n\u001b[1;32m    283\u001b[0m         mlvl_priors\u001b[39m=\u001b[39;49mmlvl_priors,\n\u001b[1;32m    284\u001b[0m         img_meta\u001b[39m=\u001b[39;49mimg_meta,\n\u001b[1;32m    285\u001b[0m         cfg\u001b[39m=\u001b[39;49mcfg,\n\u001b[1;32m    286\u001b[0m         rescale\u001b[39m=\u001b[39;49mrescale,\n\u001b[1;32m    287\u001b[0m         with_nms\u001b[39m=\u001b[39;49mwith_nms)\n\u001b[1;32m    288\u001b[0m     result_list\u001b[39m.\u001b[39mappend(results)\n\u001b[1;32m    289\u001b[0m \u001b[39mreturn\u001b[39;00m result_list\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet/models/dense_heads/rpn_head.py:233\u001b[0m, in \u001b[0;36mRPNHead._predict_by_feat_single\u001b[0;34m(self, cls_score_list, bbox_pred_list, score_factor_list, mlvl_priors, img_meta, cfg, rescale, with_nms)\u001b[0m\n\u001b[1;32m    230\u001b[0m results\u001b[39m.\u001b[39mscores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(mlvl_scores)\n\u001b[1;32m    231\u001b[0m results\u001b[39m.\u001b[39mlevel_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(level_ids)\n\u001b[0;32m--> 233\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bbox_post_process(\n\u001b[1;32m    234\u001b[0m     results\u001b[39m=\u001b[39;49mresults, cfg\u001b[39m=\u001b[39;49mcfg, rescale\u001b[39m=\u001b[39;49mrescale, img_meta\u001b[39m=\u001b[39;49mimg_meta)\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet/models/dense_heads/rpn_head.py:284\u001b[0m, in \u001b[0;36mRPNHead._bbox_post_process\u001b[0;34m(self, results, cfg, rescale, with_nms, img_meta)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mbboxes\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    283\u001b[0m     bboxes \u001b[39m=\u001b[39m get_box_tensor(results\u001b[39m.\u001b[39mbboxes)\n\u001b[0;32m--> 284\u001b[0m     det_bboxes, keep_idxs \u001b[39m=\u001b[39m batched_nms(bboxes, results\u001b[39m.\u001b[39;49mscores,\n\u001b[1;32m    285\u001b[0m                                         results\u001b[39m.\u001b[39;49mlevel_ids, cfg\u001b[39m.\u001b[39;49mnms)\n\u001b[1;32m    286\u001b[0m     results \u001b[39m=\u001b[39m results[keep_idxs]\n\u001b[1;32m    287\u001b[0m     \u001b[39m# some nms would reweight the score, such as softnms\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/ops/nms.py:303\u001b[0m, in \u001b[0;36mbatched_nms\u001b[0;34m(boxes, scores, idxs, nms_cfg, class_agnostic)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39m# Won't split to multiple nms nodes when exporting to onnx\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m boxes_for_nms\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m split_thr:\n\u001b[0;32m--> 303\u001b[0m     dets, keep \u001b[39m=\u001b[39m nms_op(boxes_for_nms, scores, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnms_cfg_)\n\u001b[1;32m    304\u001b[0m     boxes \u001b[39m=\u001b[39m boxes[keep]\n\u001b[1;32m    306\u001b[0m     \u001b[39m# This assumes `dets` has arbitrary dimensions where\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     \u001b[39m# the last dimension is score.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m     \u001b[39m# Currently it supports bounding boxes [x1, y1, x2, y2, score] or\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     \u001b[39m# rotated boxes [cx, cy, w, h, angle_radian, score].\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/utils/misc.py:395\u001b[0m, in \u001b[0;36mdeprecated_api_warning.<locals>.api_warning_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m             kwargs[dst_arg_name] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(src_arg_name)\n\u001b[1;32m    394\u001b[0m \u001b[39m# apply converted arguments to the decorated method\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m output \u001b[39m=\u001b[39m old_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    396\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/ops/nms.py:127\u001b[0m, in \u001b[0;36mnms\u001b[0;34m(boxes, scores, iou_threshold, offset, score_threshold, max_num)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39massert\u001b[39;00m boxes\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m scores\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[39massert\u001b[39;00m offset \u001b[39min\u001b[39;00m (\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m inds \u001b[39m=\u001b[39m NMSop\u001b[39m.\u001b[39;49mapply(boxes, scores, iou_threshold, offset, score_threshold,\n\u001b[1;32m    128\u001b[0m                    max_num)\n\u001b[1;32m    129\u001b[0m dets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((boxes[inds], scores[inds]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m is_numpy:\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/ops/nms.py:27\u001b[0m, in \u001b[0;36mNMSop.forward\u001b[0;34m(ctx, bboxes, scores, iou_threshold, offset, score_threshold, max_num)\u001b[0m\n\u001b[1;32m     23\u001b[0m     bboxes, scores \u001b[39m=\u001b[39m bboxes[valid_mask], scores[valid_mask]\n\u001b[1;32m     24\u001b[0m     valid_inds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnonzero(\n\u001b[1;32m     25\u001b[0m         valid_mask, as_tuple\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m inds \u001b[39m=\u001b[39m ext_module\u001b[39m.\u001b[39;49mnms(\n\u001b[1;32m     28\u001b[0m     bboxes, scores, iou_threshold\u001b[39m=\u001b[39;49m\u001b[39mfloat\u001b[39;49m(iou_threshold), offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m max_num \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     31\u001b[0m     inds \u001b[39m=\u001b[39m inds[:max_num]\n",
            "\u001b[0;31mRuntimeError\u001b[0m: nms_impl: implementation for device cuda:0 not found.\n"
          ]
        }
      ],
      "source": [
        "visualize_img(\n",
        "    img,\n",
        "    detector,\n",
        "    pose_estimator,\n",
        "    visualizer,\n",
        "    show_interval=0,\n",
        "    out_file=None)\n",
        "\n",
        "vis_result = visualizer.get_image()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "CEYxupWT3aJY",
        "outputId": "05acd979-25b1-4b18-8738-d6b9edc6bfe1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'vis_result' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[39mwith\u001b[39;00m tempfile\u001b[39m.\u001b[39mTemporaryDirectory() \u001b[39mas\u001b[39;00m tmpdir:\n\u001b[1;32m      7\u001b[0m         file_name \u001b[39m=\u001b[39m osp\u001b[39m.\u001b[39mjoin(tmpdir, \u001b[39m'\u001b[39m\u001b[39mpose_results.png\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m         cv2\u001b[39m.\u001b[39mimwrite(file_name, vis_result[:,:,::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m      9\u001b[0m         display(Image(file_name))\n\u001b[1;32m     10\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vis_result' is not defined"
          ]
        }
      ],
      "source": [
        "if local_runtime:\n",
        "    from IPython.display import Image, display\n",
        "    import tempfile\n",
        "    import os.path as osp\n",
        "    import cv2\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        file_name = osp.join(tmpdir, 'pose_results.png')\n",
        "        cv2.imwrite(file_name, vis_result[:,:,::-1])\n",
        "        display(Image(file_name))\n",
        "else:\n",
        "    cv2_imshow(vis_result[:,:,::-1]) #RGB2BGR to fit cv2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "42HG6DSNI0Ke"
      },
      "source": [
        "### Add a new dataset\n",
        "\n",
        "There are two methods to support a customized dataset in MMPose. The first one is to convert the data to a supported format (e.g. COCO) and use the corresponding dataset class (e.g. BaseCocoStyleDataset), as described in the [document](https://mmpose.readthedocs.io/en/1.x/user_guides/prepare_datasets.html). The second one is to add a new dataset class. In this tutorial, we give an example of the second method.\n",
        "\n",
        "We first download the demo dataset, which contains 100 samples (75 for training and 25 for validation) selected from COCO train2017 dataset. The annotations are stored in a different format from the original COCO format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGzSb0Rm-p3V",
        "outputId": "2e7ec2ba-88e1-490f-cd5a-66ef06ec3e52"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "%mkdir data\n",
        "%cd data\n",
        "!wget https://download.openmmlab.com/mmpose/datasets/coco_tiny.tar\n",
        "!tar -xf coco_tiny.tar\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL6S62JWJls0",
        "outputId": "fe4cf7c9-5a8c-4542-f0b1-fe01908ca3e4"
      },
      "outputs": [],
      "source": [
        "# check the directory structure\n",
        "!apt-get -q install tree\n",
        "!tree data/coco_tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl09rtA4Jn5b",
        "outputId": "e94e84ea-7192-4d2f-9747-716931953d6d"
      },
      "outputs": [],
      "source": [
        "# check the annotation format\n",
        "import json\n",
        "import pprint\n",
        "\n",
        "anns = json.load(open('data/coco_tiny/train.json'))\n",
        "\n",
        "print(type(anns), len(anns))\n",
        "pprint.pprint(anns[0], compact=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H-dMbjgnJzbH"
      },
      "source": [
        "After downloading the data, we implement a new dataset class to load data samples for model training and validation. Assume that we are going to train a top-down pose estimation model, the new dataset class inherits `BaseCocoStyleDataset`.\n",
        "\n",
        "We have already implemented a `CocoDataset` so that we can take it as an example."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jCu4npV2rl_Q"
      },
      "source": [
        "#### Note\n",
        "If you meet the following error:\n",
        "```shell\n",
        "AssertionError: class `PoseLocalVisualizer` in mmpose/visualization/local_visualizer.py: <class 'mmpose.visualization.local_visualizer.PoseLocalVisualizer'> instance named of visualizer has been created, the method `get_instance` should not access any other arguments\n",
        "```\n",
        "Please reboot your jupyter kernel and start running from here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I66Pi5Er94J"
      },
      "outputs": [],
      "source": [
        "%cd mmpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRNq50dytJki"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) OpenMMLab. All rights reserved.\n",
        "import json\n",
        "import os.path as osp\n",
        "from typing import Callable, List, Optional, Sequence, Union\n",
        "\n",
        "import numpy as np\n",
        "from mmengine.utils import check_file_exist\n",
        "\n",
        "from mmpose.registry import DATASETS\n",
        "from mmpose.datasets.datasets.base import BaseCocoStyleDataset\n",
        "\n",
        "\n",
        "@DATASETS.register_module()\n",
        "class TinyCocoDataset(BaseCocoStyleDataset):\n",
        "    METAINFO: dict = dict(from_file='configs/_base_/datasets/coco.py')\n",
        "\n",
        "    def _load_annotations(self) -> List[dict]:\n",
        "        \"\"\"Load data from annotations in MPII format.\"\"\"\n",
        "\n",
        "        check_file_exist(self.ann_file)\n",
        "        with open(self.ann_file) as anno_file:\n",
        "            anns = json.load(anno_file)\n",
        "\n",
        "        data_list = []\n",
        "        ann_id = 0\n",
        "\n",
        "        for idx, ann in enumerate(anns):\n",
        "            img_h, img_w = ann['image_size']\n",
        "\n",
        "            # get bbox in shape [1, 4], formatted as xywh\n",
        "            x, y, w, h = ann['bbox']\n",
        "            x1 = np.clip(x, 0, img_w - 1)\n",
        "            y1 = np.clip(y, 0, img_h - 1)\n",
        "            x2 = np.clip(x + w, 0, img_w - 1)\n",
        "            y2 = np.clip(y + h, 0, img_h - 1)\n",
        "\n",
        "            bbox = np.array([x1, y1, x2, y2], dtype=np.float32).reshape(1, 4)\n",
        "\n",
        "            # load keypoints in shape [1, K, 2] and keypoints_visible in [1, K]\n",
        "            joints_3d = np.array(ann['keypoints']).reshape(1, -1, 3)\n",
        "            num_joints = joints_3d.shape[1]\n",
        "            keypoints = np.zeros((1, num_joints, 2), dtype=np.float32)\n",
        "            keypoints[:, :, :2] = joints_3d[:, :, :2]\n",
        "            keypoints_visible = np.minimum(1, joints_3d[:, :, 2:3])\n",
        "            keypoints_visible = keypoints_visible.reshape(1, -1)\n",
        "\n",
        "            data_info = {\n",
        "                'id': ann_id,\n",
        "                'img_id': int(ann['image_file'].split('.')[0]),\n",
        "                'img_path': osp.join(self.data_prefix['img'], ann['image_file']),\n",
        "                'bbox': bbox,\n",
        "                'bbox_score': np.ones(1, dtype=np.float32),\n",
        "                'keypoints': keypoints,\n",
        "                'keypoints_visible': keypoints_visible,\n",
        "            }\n",
        "\n",
        "            data_list.append(data_info)\n",
        "            ann_id = ann_id + 1\n",
        "\n",
        "        return data_list, None\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UmGitQZkUnom"
      },
      "source": [
        "### Create a config file\n",
        "\n",
        "In the next step, we create a config file which configures the model, dataset and runtime settings. More information can be found at [Configs](https://mmpose.readthedocs.io/en/1.x/user_guides/configs.html). A common practice to create a config file is deriving from a existing one. In this tutorial, we load a config file that trains a HRNet on COCO dataset, and modify it to adapt to the COCOTiny dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMbVVHPXK87s",
        "outputId": "a23a1ed9-a2ee-4a6a-93da-3c1968c8a2ec"
      },
      "outputs": [],
      "source": [
        "from mmengine import Config\n",
        "\n",
        "cfg = Config.fromfile(\n",
        "    './configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192.py'\n",
        ")\n",
        "\n",
        "# set basic configs\n",
        "cfg.data_root = 'data/coco_tiny'\n",
        "cfg.work_dir = 'work_dirs/hrnet_w32_coco_tiny_256x192'\n",
        "cfg.randomness = dict(seed=0)\n",
        "\n",
        "# set log interval\n",
        "cfg.train_cfg.val_interval = 1\n",
        "\n",
        "# set num of epoch\n",
        "cfg.train_cfg.max_epochs = 40\n",
        "\n",
        "# set optimizer\n",
        "cfg.optim_wrapper = dict(optimizer=dict(\n",
        "    type='Adam',\n",
        "    lr=5e-4,\n",
        "))\n",
        "\n",
        "# set learning rate policy\n",
        "cfg.param_scheduler = [\n",
        "    dict(\n",
        "        type='LinearLR', begin=0, end=10, start_factor=0.001,\n",
        "        by_epoch=False),  # warm-up\n",
        "    dict(\n",
        "        type='MultiStepLR',\n",
        "        begin=0,\n",
        "        end=cfg.train_cfg.max_epochs,\n",
        "        milestones=[17, 35],\n",
        "        gamma=0.1,\n",
        "        by_epoch=True)\n",
        "]\n",
        "\n",
        "\n",
        "# set batch size\n",
        "cfg.train_dataloader.batch_size = 16\n",
        "cfg.val_dataloader.batch_size = 16\n",
        "cfg.test_dataloader.batch_size = 16\n",
        "\n",
        "# set dataset configs\n",
        "cfg.dataset_type = 'TinyCocoDataset'\n",
        "cfg.train_dataloader.dataset.type = cfg.dataset_type\n",
        "cfg.train_dataloader.dataset.ann_file = 'train.json'\n",
        "cfg.train_dataloader.dataset.data_root = cfg.data_root\n",
        "cfg.train_dataloader.dataset.data_prefix = dict(img='images/')\n",
        "\n",
        "\n",
        "cfg.val_dataloader.dataset.type = cfg.dataset_type\n",
        "cfg.val_dataloader.dataset.bbox_file = None\n",
        "cfg.val_dataloader.dataset.ann_file = 'val.json'\n",
        "cfg.val_dataloader.dataset.data_root = cfg.data_root\n",
        "cfg.val_dataloader.dataset.data_prefix = dict(img='images/')\n",
        "\n",
        "cfg.test_dataloader.dataset.type = cfg.dataset_type\n",
        "cfg.test_dataloader.dataset.bbox_file = None\n",
        "cfg.test_dataloader.dataset.ann_file = 'val.json'\n",
        "cfg.test_dataloader.dataset.data_root = cfg.data_root\n",
        "cfg.test_dataloader.dataset.data_prefix = dict(img='images/')\n",
        "\n",
        "# set evaluator\n",
        "cfg.val_evaluator = dict(type='PCKAccuracy')\n",
        "cfg.test_evaluator = cfg.val_evaluator\n",
        "\n",
        "cfg.default_hooks.checkpoint.save_best = 'PCK'\n",
        "cfg.default_hooks.checkpoint.max_keep_ckpts = 1\n",
        "\n",
        "print(cfg.pretty_text)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UlD8iDZehE2S"
      },
      "source": [
        "or you can create a config file like follows:\n",
        "```Python3\n",
        "_base_ = ['../../../_base_/default_runtime.py']\n",
        "\n",
        "# runtime\n",
        "train_cfg = dict(max_epochs=40, val_interval=1)\n",
        "\n",
        "# optimizer\n",
        "optim_wrapper = dict(optimizer=dict(\n",
        "    type='Adam',\n",
        "    lr=5e-4,\n",
        "))\n",
        "\n",
        "# learning policy\n",
        "param_scheduler = [\n",
        "    dict(\n",
        "        type='LinearLR', begin=0, end=500, start_factor=0.001,\n",
        "        by_epoch=False),  # warm-up\n",
        "    dict(\n",
        "        type='MultiStepLR',\n",
        "        begin=0,\n",
        "        end=train_cfg.max_epochs,\n",
        "        milestones=[17, 35],\n",
        "        gamma=0.1,\n",
        "        by_epoch=True)\n",
        "]\n",
        "\n",
        "# automatically scaling LR based on the actual training batch size\n",
        "auto_scale_lr = dict(base_batch_size=512)\n",
        "\n",
        "# codec settings\n",
        "codec = dict(\n",
        "    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)\n",
        "\n",
        "# model settings\n",
        "model = dict(\n",
        "    type='TopdownPoseEstimator',\n",
        "    data_preprocessor=dict(\n",
        "        type='PoseDataPreprocessor',\n",
        "        mean=[123.675, 116.28, 103.53],\n",
        "        std=[58.395, 57.12, 57.375],\n",
        "        bgr_to_rgb=True),\n",
        "    backbone=dict(\n",
        "        type='HRNet',\n",
        "        in_channels=3,\n",
        "        extra=dict(\n",
        "            stage1=dict(\n",
        "                num_modules=1,\n",
        "                num_branches=1,\n",
        "                block='BOTTLENECK',\n",
        "                num_blocks=(4, ),\n",
        "                num_channels=(64, )),\n",
        "            stage2=dict(\n",
        "                num_modules=1,\n",
        "                num_branches=2,\n",
        "                block='BASIC',\n",
        "                num_blocks=(4, 4),\n",
        "                num_channels=(32, 64)),\n",
        "            stage3=dict(\n",
        "                num_modules=4,\n",
        "                num_branches=3,\n",
        "                block='BASIC',\n",
        "                num_blocks=(4, 4, 4),\n",
        "                num_channels=(32, 64, 128)),\n",
        "            stage4=dict(\n",
        "                num_modules=3,\n",
        "                num_branches=4,\n",
        "                block='BASIC',\n",
        "                num_blocks=(4, 4, 4, 4),\n",
        "                num_channels=(32, 64, 128, 256))),\n",
        "        init_cfg=dict(\n",
        "            type='Pretrained',\n",
        "            checkpoint='https://download.openmmlab.com/mmpose/'\n",
        "            'pretrain_models/hrnet_w32-36af842e.pth'),\n",
        "    ),\n",
        "    head=dict(\n",
        "        type='HeatmapHead',\n",
        "        in_channels=32,\n",
        "        out_channels=17,\n",
        "        deconv_out_channels=None,\n",
        "        loss=dict(type='KeypointMSELoss', use_target_weight=True),\n",
        "        decoder=codec),\n",
        "    test_cfg=dict(\n",
        "        flip_test=True,\n",
        "        flip_mode='heatmap',\n",
        "        shift_heatmap=True,\n",
        "    ))\n",
        "\n",
        "# base dataset settings\n",
        "dataset_type = 'TinyCocoDataset'\n",
        "data_mode = 'topdown'\n",
        "data_root = 'data/coco_tiny'\n",
        "work_dir = 'work_dirs/hrnet_w32_coco_tiny_256x192'\n",
        "randomness = dict(seed=0)\n",
        "\n",
        "# pipelines\n",
        "train_pipeline = [\n",
        "    dict(type='LoadImage'),\n",
        "    dict(type='GetBBoxCenterScale'),\n",
        "    dict(type='RandomFlip', direction='horizontal'),\n",
        "    dict(type='RandomHalfBody'),\n",
        "    dict(type='RandomBBoxTransform'),\n",
        "    dict(type='TopdownAffine', input_size=codec['input_size']),\n",
        "    dict(type='GenerateTarget', target_type='heatmap', encoder=codec),\n",
        "    dict(type='PackPoseInputs')\n",
        "]\n",
        "test_pipeline = [\n",
        "    dict(type='LoadImage'),\n",
        "    dict(type='GetBBoxCenterScale'),\n",
        "    dict(type='TopdownAffine', input_size=codec['input_size']),\n",
        "    dict(type='PackPoseInputs')\n",
        "]\n",
        "\n",
        "# data loaders\n",
        "train_dataloader = dict(\n",
        "    batch_size=16,\n",
        "    num_workers=2,\n",
        "    persistent_workers=True,\n",
        "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
        "    dataset=dict(\n",
        "        type=dataset_type,\n",
        "        data_root=data_root,\n",
        "        data_mode=data_mode,\n",
        "        ann_file='train.json',\n",
        "        data_prefix=dict(img='images/'),\n",
        "        pipeline=train_pipeline,\n",
        "    ))\n",
        "val_dataloader = dict(\n",
        "    batch_size=16,\n",
        "    num_workers=2,\n",
        "    persistent_workers=True,\n",
        "    drop_last=False,\n",
        "    sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),\n",
        "    dataset=dict(\n",
        "        type=dataset_type,\n",
        "        data_root=data_root,\n",
        "        data_mode=data_mode,\n",
        "        ann_file='val.json',\n",
        "        data_prefix=dict(img='images/'),\n",
        "        test_mode=True,\n",
        "        pipeline=test_pipeline,\n",
        "    ))\n",
        "test_dataloader = val_dataloader\n",
        "\n",
        "# evaluators\n",
        "val_evaluator = dict(\n",
        "    type='PCKAccuracy')\n",
        "test_evaluator = val_evaluator\n",
        "\n",
        "# hooks\n",
        "default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ChVqB1oYncmo"
      },
      "source": [
        "### Train and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2a079d9c0b9845318e6c612ca9601b86",
            "3554753622334094961a47daf9362c59",
            "08e0412b8dd54d28a26c232e75ea6088",
            "558a9420b0b34be2a2ca8a8b8af9cbfc",
            "a9bd3e477f07449788f0e95e3cd13ddc",
            "5b2ee1f3e78d4cd993009d04baf76b24",
            "a3e5aa31c3f644b5a677ec49fe2e0832",
            "d2ee56f920a245d9875de8e37596a5c8",
            "b5f8c86d48a04afa997fc137e1acd716",
            "1c1b09d91dec4e3dadefe953daf50745",
            "6af448aebdb744b98a2807f66b1d6e5d"
          ]
        },
        "id": "Ab3xsUdPlXuJ",
        "outputId": "c07394b8-21f4-4766-af2b-87d2caa6e74c"
      },
      "outputs": [],
      "source": [
        "from mmengine.config import Config, DictAction\n",
        "from mmengine.runner import Runner\n",
        "\n",
        "# set preprocess configs to model\n",
        "cfg.model.setdefault('data_preprocessor', cfg.get('preprocess_cfg', {}))\n",
        "\n",
        "# build the runner from config\n",
        "runner = Runner.from_cfg(cfg)\n",
        "\n",
        "# start training\n",
        "runner.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sdLwcaojhE2T"
      },
      "source": [
        "#### Note\n",
        "The recommended best practice is to convert your customized data into COCO format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJyteZNGqwNk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "dev2.0",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "vscode": {
      "interpreter": {
        "hash": "383ba00087b5a9caebf3648b758a31e474cc01be975489b58f119fa4bc17e1f8"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08e0412b8dd54d28a26c232e75ea6088": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ee56f920a245d9875de8e37596a5c8",
            "max": 132594821,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5f8c86d48a04afa997fc137e1acd716",
            "value": 132594821
          }
        },
        "1c1b09d91dec4e3dadefe953daf50745": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a079d9c0b9845318e6c612ca9601b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3554753622334094961a47daf9362c59",
              "IPY_MODEL_08e0412b8dd54d28a26c232e75ea6088",
              "IPY_MODEL_558a9420b0b34be2a2ca8a8b8af9cbfc"
            ],
            "layout": "IPY_MODEL_a9bd3e477f07449788f0e95e3cd13ddc"
          }
        },
        "3554753622334094961a47daf9362c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b2ee1f3e78d4cd993009d04baf76b24",
            "placeholder": "",
            "style": "IPY_MODEL_a3e5aa31c3f644b5a677ec49fe2e0832",
            "value": "100%"
          }
        },
        "558a9420b0b34be2a2ca8a8b8af9cbfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c1b09d91dec4e3dadefe953daf50745",
            "placeholder": "",
            "style": "IPY_MODEL_6af448aebdb744b98a2807f66b1d6e5d",
            "value": " 126M/126M [00:14&lt;00:00, 9.32MB/s]"
          }
        },
        "5b2ee1f3e78d4cd993009d04baf76b24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6af448aebdb744b98a2807f66b1d6e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3e5aa31c3f644b5a677ec49fe2e0832": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9bd3e477f07449788f0e95e3cd13ddc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f8c86d48a04afa997fc137e1acd716": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2ee56f920a245d9875de8e37596a5c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
